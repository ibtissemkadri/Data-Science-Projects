{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_cnn_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VbUeXqD8BAag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87bcd339-85a6-45f3-afe1-c83bcefff3e8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import  numpy  as  np\n",
        "import  pandas  as pd\n",
        "import  pickle\n",
        "from  collections import defaultdict\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, BatchNormalization, Activation, LSTM, Bidirectional\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4VLA7maU-Gji",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading and processing data"
      ]
    },
    {
      "metadata": {
        "id": "aDlhQthDBQmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "728b511b-c221-429d-d282-592560db8115"
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
        "                                                         num_words=None,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=None,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=113,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h0RuytOxB1Bs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c039491a-4f08-42dd-eaa4-28fd369c7a81"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "_BCQCx3oCbXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_classes=46\n",
        "y_train=to_categorical(y_train, nb_classes)\n",
        "y_test=to_categorical(y_test, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPJpYK0pDSI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ced91dc-d2b6-49e9-a015-52e10bd08787"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "ofjbdicWEdME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ae9843bf-bb32-411f-e581-50c1b2043d89"
      },
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "print('Number of Unique Tokens',len(word_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "Number of Unique Tokens 30979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_h1O0n1ObkrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        },
        "outputId": "0ef5956e-e5fa-4dee-fe9e-aeb143e6e9cc"
      },
      "cell_type": "code",
      "source": [
        "sorted(word_index.values(), reverse=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[30979,\n",
              " 30978,\n",
              " 30977,\n",
              " 30976,\n",
              " 30975,\n",
              " 30974,\n",
              " 30973,\n",
              " 30972,\n",
              " 30971,\n",
              " 30970,\n",
              " 30969,\n",
              " 30968,\n",
              " 30967,\n",
              " 30966,\n",
              " 30965,\n",
              " 30964,\n",
              " 30963,\n",
              " 30962,\n",
              " 30961,\n",
              " 30960,\n",
              " 30959,\n",
              " 30958,\n",
              " 30957,\n",
              " 30956,\n",
              " 30955,\n",
              " 30954,\n",
              " 30953,\n",
              " 30952,\n",
              " 30951,\n",
              " 30950,\n",
              " 30949,\n",
              " 30948,\n",
              " 30947,\n",
              " 30946,\n",
              " 30945,\n",
              " 30944,\n",
              " 30943,\n",
              " 30942,\n",
              " 30941,\n",
              " 30940,\n",
              " 30939,\n",
              " 30938,\n",
              " 30937,\n",
              " 30936,\n",
              " 30935,\n",
              " 30934,\n",
              " 30933,\n",
              " 30932,\n",
              " 30931,\n",
              " 30930,\n",
              " 30929,\n",
              " 30928,\n",
              " 30927,\n",
              " 30926,\n",
              " 30925,\n",
              " 30924,\n",
              " 30923,\n",
              " 30922,\n",
              " 30921,\n",
              " 30920,\n",
              " 30919,\n",
              " 30918,\n",
              " 30917,\n",
              " 30916,\n",
              " 30915,\n",
              " 30914,\n",
              " 30913,\n",
              " 30912,\n",
              " 30911,\n",
              " 30910,\n",
              " 30909,\n",
              " 30908,\n",
              " 30907,\n",
              " 30906,\n",
              " 30905,\n",
              " 30904,\n",
              " 30903,\n",
              " 30902,\n",
              " 30901,\n",
              " 30900,\n",
              " 30899,\n",
              " 30898,\n",
              " 30897,\n",
              " 30896,\n",
              " 30895,\n",
              " 30894,\n",
              " 30893,\n",
              " 30892,\n",
              " 30891,\n",
              " 30890,\n",
              " 30889,\n",
              " 30888,\n",
              " 30887,\n",
              " 30886,\n",
              " 30885,\n",
              " 30884,\n",
              " 30883,\n",
              " 30882,\n",
              " 30881,\n",
              " 30880,\n",
              " 30879,\n",
              " 30878,\n",
              " 30877,\n",
              " 30876,\n",
              " 30875,\n",
              " 30874,\n",
              " 30873,\n",
              " 30872,\n",
              " 30871,\n",
              " 30870,\n",
              " 30869,\n",
              " 30868,\n",
              " 30867,\n",
              " 30866,\n",
              " 30865,\n",
              " 30864,\n",
              " 30863,\n",
              " 30862,\n",
              " 30861,\n",
              " 30860,\n",
              " 30859,\n",
              " 30858,\n",
              " 30857,\n",
              " 30856,\n",
              " 30855,\n",
              " 30854,\n",
              " 30853,\n",
              " 30852,\n",
              " 30851,\n",
              " 30850,\n",
              " 30849,\n",
              " 30848,\n",
              " 30847,\n",
              " 30846,\n",
              " 30845,\n",
              " 30844,\n",
              " 30843,\n",
              " 30842,\n",
              " 30841,\n",
              " 30840,\n",
              " 30839,\n",
              " 30838,\n",
              " 30837,\n",
              " 30836,\n",
              " 30835,\n",
              " 30834,\n",
              " 30833,\n",
              " 30832,\n",
              " 30831,\n",
              " 30830,\n",
              " 30829,\n",
              " 30828,\n",
              " 30827,\n",
              " 30826,\n",
              " 30825,\n",
              " 30824,\n",
              " 30823,\n",
              " 30822,\n",
              " 30821,\n",
              " 30820,\n",
              " 30819,\n",
              " 30818,\n",
              " 30817,\n",
              " 30816,\n",
              " 30815,\n",
              " 30814,\n",
              " 30813,\n",
              " 30812,\n",
              " 30811,\n",
              " 30810,\n",
              " 30809,\n",
              " 30808,\n",
              " 30807,\n",
              " 30806,\n",
              " 30805,\n",
              " 30804,\n",
              " 30803,\n",
              " 30802,\n",
              " 30801,\n",
              " 30800,\n",
              " 30799,\n",
              " 30798,\n",
              " 30797,\n",
              " 30796,\n",
              " 30795,\n",
              " 30794,\n",
              " 30793,\n",
              " 30792,\n",
              " 30791,\n",
              " 30790,\n",
              " 30789,\n",
              " 30788,\n",
              " 30787,\n",
              " 30786,\n",
              " 30785,\n",
              " 30784,\n",
              " 30783,\n",
              " 30782,\n",
              " 30781,\n",
              " 30780,\n",
              " 30779,\n",
              " 30778,\n",
              " 30777,\n",
              " 30776,\n",
              " 30775,\n",
              " 30774,\n",
              " 30773,\n",
              " 30772,\n",
              " 30771,\n",
              " 30770,\n",
              " 30769,\n",
              " 30768,\n",
              " 30767,\n",
              " 30766,\n",
              " 30765,\n",
              " 30764,\n",
              " 30763,\n",
              " 30762,\n",
              " 30761,\n",
              " 30760,\n",
              " 30759,\n",
              " 30758,\n",
              " 30757,\n",
              " 30756,\n",
              " 30755,\n",
              " 30754,\n",
              " 30753,\n",
              " 30752,\n",
              " 30751,\n",
              " 30750,\n",
              " 30749,\n",
              " 30748,\n",
              " 30747,\n",
              " 30746,\n",
              " 30745,\n",
              " 30744,\n",
              " 30743,\n",
              " 30742,\n",
              " 30741,\n",
              " 30740,\n",
              " 30739,\n",
              " 30738,\n",
              " 30737,\n",
              " 30736,\n",
              " 30735,\n",
              " 30734,\n",
              " 30733,\n",
              " 30732,\n",
              " 30731,\n",
              " 30730,\n",
              " 30729,\n",
              " 30728,\n",
              " 30727,\n",
              " 30726,\n",
              " 30725,\n",
              " 30724,\n",
              " 30723,\n",
              " 30722,\n",
              " 30721,\n",
              " 30720,\n",
              " 30719,\n",
              " 30718,\n",
              " 30717,\n",
              " 30716,\n",
              " 30715,\n",
              " 30714,\n",
              " 30713,\n",
              " 30712,\n",
              " 30711,\n",
              " 30710,\n",
              " 30709,\n",
              " 30708,\n",
              " 30707,\n",
              " 30706,\n",
              " 30705,\n",
              " 30704,\n",
              " 30703,\n",
              " 30702,\n",
              " 30701,\n",
              " 30700,\n",
              " 30699,\n",
              " 30698,\n",
              " 30697,\n",
              " 30696,\n",
              " 30695,\n",
              " 30694,\n",
              " 30693,\n",
              " 30692,\n",
              " 30691,\n",
              " 30690,\n",
              " 30689,\n",
              " 30688,\n",
              " 30687,\n",
              " 30686,\n",
              " 30685,\n",
              " 30684,\n",
              " 30683,\n",
              " 30682,\n",
              " 30681,\n",
              " 30680,\n",
              " 30679,\n",
              " 30678,\n",
              " 30677,\n",
              " 30676,\n",
              " 30675,\n",
              " 30674,\n",
              " 30673,\n",
              " 30672,\n",
              " 30671,\n",
              " 30670,\n",
              " 30669,\n",
              " 30668,\n",
              " 30667,\n",
              " 30666,\n",
              " 30665,\n",
              " 30664,\n",
              " 30663,\n",
              " 30662,\n",
              " 30661,\n",
              " 30660,\n",
              " 30659,\n",
              " 30658,\n",
              " 30657,\n",
              " 30656,\n",
              " 30655,\n",
              " 30654,\n",
              " 30653,\n",
              " 30652,\n",
              " 30651,\n",
              " 30650,\n",
              " 30649,\n",
              " 30648,\n",
              " 30647,\n",
              " 30646,\n",
              " 30645,\n",
              " 30644,\n",
              " 30643,\n",
              " 30642,\n",
              " 30641,\n",
              " 30640,\n",
              " 30639,\n",
              " 30638,\n",
              " 30637,\n",
              " 30636,\n",
              " 30635,\n",
              " 30634,\n",
              " 30633,\n",
              " 30632,\n",
              " 30631,\n",
              " 30630,\n",
              " 30629,\n",
              " 30628,\n",
              " 30627,\n",
              " 30626,\n",
              " 30625,\n",
              " 30624,\n",
              " 30623,\n",
              " 30622,\n",
              " 30621,\n",
              " 30620,\n",
              " 30619,\n",
              " 30618,\n",
              " 30617,\n",
              " 30616,\n",
              " 30615,\n",
              " 30614,\n",
              " 30613,\n",
              " 30612,\n",
              " 30611,\n",
              " 30610,\n",
              " 30609,\n",
              " 30608,\n",
              " 30607,\n",
              " 30606,\n",
              " 30605,\n",
              " 30604,\n",
              " 30603,\n",
              " 30602,\n",
              " 30601,\n",
              " 30600,\n",
              " 30599,\n",
              " 30598,\n",
              " 30597,\n",
              " 30596,\n",
              " 30595,\n",
              " 30594,\n",
              " 30593,\n",
              " 30592,\n",
              " 30591,\n",
              " 30590,\n",
              " 30589,\n",
              " 30588,\n",
              " 30587,\n",
              " 30586,\n",
              " 30585,\n",
              " 30584,\n",
              " 30583,\n",
              " 30582,\n",
              " 30581,\n",
              " 30580,\n",
              " 30579,\n",
              " 30578,\n",
              " 30577,\n",
              " 30576,\n",
              " 30575,\n",
              " 30574,\n",
              " 30573,\n",
              " 30572,\n",
              " 30571,\n",
              " 30570,\n",
              " 30569,\n",
              " 30568,\n",
              " 30567,\n",
              " 30566,\n",
              " 30565,\n",
              " 30564,\n",
              " 30563,\n",
              " 30562,\n",
              " 30561,\n",
              " 30560,\n",
              " 30559,\n",
              " 30558,\n",
              " 30557,\n",
              " 30556,\n",
              " 30555,\n",
              " 30554,\n",
              " 30553,\n",
              " 30552,\n",
              " 30551,\n",
              " 30550,\n",
              " 30549,\n",
              " 30548,\n",
              " 30547,\n",
              " 30546,\n",
              " 30545,\n",
              " 30544,\n",
              " 30543,\n",
              " 30542,\n",
              " 30541,\n",
              " 30540,\n",
              " 30539,\n",
              " 30538,\n",
              " 30537,\n",
              " 30536,\n",
              " 30535,\n",
              " 30534,\n",
              " 30533,\n",
              " 30532,\n",
              " 30531,\n",
              " 30530,\n",
              " 30529,\n",
              " 30528,\n",
              " 30527,\n",
              " 30526,\n",
              " 30525,\n",
              " 30524,\n",
              " 30523,\n",
              " 30522,\n",
              " 30521,\n",
              " 30520,\n",
              " 30519,\n",
              " 30518,\n",
              " 30517,\n",
              " 30516,\n",
              " 30515,\n",
              " 30514,\n",
              " 30513,\n",
              " 30512,\n",
              " 30511,\n",
              " 30510,\n",
              " 30509,\n",
              " 30508,\n",
              " 30507,\n",
              " 30506,\n",
              " 30505,\n",
              " 30504,\n",
              " 30503,\n",
              " 30502,\n",
              " 30501,\n",
              " 30500,\n",
              " 30499,\n",
              " 30498,\n",
              " 30497,\n",
              " 30496,\n",
              " 30495,\n",
              " 30494,\n",
              " 30493,\n",
              " 30492,\n",
              " 30491,\n",
              " 30490,\n",
              " 30489,\n",
              " 30488,\n",
              " 30487,\n",
              " 30486,\n",
              " 30485,\n",
              " 30484,\n",
              " 30483,\n",
              " 30482,\n",
              " 30481,\n",
              " 30480,\n",
              " 30479,\n",
              " 30478,\n",
              " 30477,\n",
              " 30476,\n",
              " 30475,\n",
              " 30474,\n",
              " 30473,\n",
              " 30472,\n",
              " 30471,\n",
              " 30470,\n",
              " 30469,\n",
              " 30468,\n",
              " 30467,\n",
              " 30466,\n",
              " 30465,\n",
              " 30464,\n",
              " 30463,\n",
              " 30462,\n",
              " 30461,\n",
              " 30460,\n",
              " 30459,\n",
              " 30458,\n",
              " 30457,\n",
              " 30456,\n",
              " 30455,\n",
              " 30454,\n",
              " 30453,\n",
              " 30452,\n",
              " 30451,\n",
              " 30450,\n",
              " 30449,\n",
              " 30448,\n",
              " 30447,\n",
              " 30446,\n",
              " 30445,\n",
              " 30444,\n",
              " 30443,\n",
              " 30442,\n",
              " 30441,\n",
              " 30440,\n",
              " 30439,\n",
              " 30438,\n",
              " 30437,\n",
              " 30436,\n",
              " 30435,\n",
              " 30434,\n",
              " 30433,\n",
              " 30432,\n",
              " 30431,\n",
              " 30430,\n",
              " 30429,\n",
              " 30428,\n",
              " 30427,\n",
              " 30426,\n",
              " 30425,\n",
              " 30424,\n",
              " 30423,\n",
              " 30422,\n",
              " 30421,\n",
              " 30420,\n",
              " 30419,\n",
              " 30418,\n",
              " 30417,\n",
              " 30416,\n",
              " 30415,\n",
              " 30414,\n",
              " 30413,\n",
              " 30412,\n",
              " 30411,\n",
              " 30410,\n",
              " 30409,\n",
              " 30408,\n",
              " 30407,\n",
              " 30406,\n",
              " 30405,\n",
              " 30404,\n",
              " 30403,\n",
              " 30402,\n",
              " 30401,\n",
              " 30400,\n",
              " 30399,\n",
              " 30398,\n",
              " 30397,\n",
              " 30396,\n",
              " 30395,\n",
              " 30394,\n",
              " 30393,\n",
              " 30392,\n",
              " 30391,\n",
              " 30390,\n",
              " 30389,\n",
              " 30388,\n",
              " 30387,\n",
              " 30386,\n",
              " 30385,\n",
              " 30384,\n",
              " 30383,\n",
              " 30382,\n",
              " 30381,\n",
              " 30380,\n",
              " 30379,\n",
              " 30378,\n",
              " 30377,\n",
              " 30376,\n",
              " 30375,\n",
              " 30374,\n",
              " 30373,\n",
              " 30372,\n",
              " 30371,\n",
              " 30370,\n",
              " 30369,\n",
              " 30368,\n",
              " 30367,\n",
              " 30366,\n",
              " 30365,\n",
              " 30364,\n",
              " 30363,\n",
              " 30362,\n",
              " 30361,\n",
              " 30360,\n",
              " 30359,\n",
              " 30358,\n",
              " 30357,\n",
              " 30356,\n",
              " 30355,\n",
              " 30354,\n",
              " 30353,\n",
              " 30352,\n",
              " 30351,\n",
              " 30350,\n",
              " 30349,\n",
              " 30348,\n",
              " 30347,\n",
              " 30346,\n",
              " 30345,\n",
              " 30344,\n",
              " 30343,\n",
              " 30342,\n",
              " 30341,\n",
              " 30340,\n",
              " 30339,\n",
              " 30338,\n",
              " 30337,\n",
              " 30336,\n",
              " 30335,\n",
              " 30334,\n",
              " 30333,\n",
              " 30332,\n",
              " 30331,\n",
              " 30330,\n",
              " 30329,\n",
              " 30328,\n",
              " 30327,\n",
              " 30326,\n",
              " 30325,\n",
              " 30324,\n",
              " 30323,\n",
              " 30322,\n",
              " 30321,\n",
              " 30320,\n",
              " 30319,\n",
              " 30318,\n",
              " 30317,\n",
              " 30316,\n",
              " 30315,\n",
              " 30314,\n",
              " 30313,\n",
              " 30312,\n",
              " 30311,\n",
              " 30310,\n",
              " 30309,\n",
              " 30308,\n",
              " 30307,\n",
              " 30306,\n",
              " 30305,\n",
              " 30304,\n",
              " 30303,\n",
              " 30302,\n",
              " 30301,\n",
              " 30300,\n",
              " 30299,\n",
              " 30298,\n",
              " 30297,\n",
              " 30296,\n",
              " 30295,\n",
              " 30294,\n",
              " 30293,\n",
              " 30292,\n",
              " 30291,\n",
              " 30290,\n",
              " 30289,\n",
              " 30288,\n",
              " 30287,\n",
              " 30286,\n",
              " 30285,\n",
              " 30284,\n",
              " 30283,\n",
              " 30282,\n",
              " 30281,\n",
              " 30280,\n",
              " 30279,\n",
              " 30278,\n",
              " 30277,\n",
              " 30276,\n",
              " 30275,\n",
              " 30274,\n",
              " 30273,\n",
              " 30272,\n",
              " 30271,\n",
              " 30270,\n",
              " 30269,\n",
              " 30268,\n",
              " 30267,\n",
              " 30266,\n",
              " 30265,\n",
              " 30264,\n",
              " 30263,\n",
              " 30262,\n",
              " 30261,\n",
              " 30260,\n",
              " 30259,\n",
              " 30258,\n",
              " 30257,\n",
              " 30256,\n",
              " 30255,\n",
              " 30254,\n",
              " 30253,\n",
              " 30252,\n",
              " 30251,\n",
              " 30250,\n",
              " 30249,\n",
              " 30248,\n",
              " 30247,\n",
              " 30246,\n",
              " 30245,\n",
              " 30244,\n",
              " 30243,\n",
              " 30242,\n",
              " 30241,\n",
              " 30240,\n",
              " 30239,\n",
              " 30238,\n",
              " 30237,\n",
              " 30236,\n",
              " 30235,\n",
              " 30234,\n",
              " 30233,\n",
              " 30232,\n",
              " 30231,\n",
              " 30230,\n",
              " 30229,\n",
              " 30228,\n",
              " 30227,\n",
              " 30226,\n",
              " 30225,\n",
              " 30224,\n",
              " 30223,\n",
              " 30222,\n",
              " 30221,\n",
              " 30220,\n",
              " 30219,\n",
              " 30218,\n",
              " 30217,\n",
              " 30216,\n",
              " 30215,\n",
              " 30214,\n",
              " 30213,\n",
              " 30212,\n",
              " 30211,\n",
              " 30210,\n",
              " 30209,\n",
              " 30208,\n",
              " 30207,\n",
              " 30206,\n",
              " 30205,\n",
              " 30204,\n",
              " 30203,\n",
              " 30202,\n",
              " 30201,\n",
              " 30200,\n",
              " 30199,\n",
              " 30198,\n",
              " 30197,\n",
              " 30196,\n",
              " 30195,\n",
              " 30194,\n",
              " 30193,\n",
              " 30192,\n",
              " 30191,\n",
              " 30190,\n",
              " 30189,\n",
              " 30188,\n",
              " 30187,\n",
              " 30186,\n",
              " 30185,\n",
              " 30184,\n",
              " 30183,\n",
              " 30182,\n",
              " 30181,\n",
              " 30180,\n",
              " 30179,\n",
              " 30178,\n",
              " 30177,\n",
              " 30176,\n",
              " 30175,\n",
              " 30174,\n",
              " 30173,\n",
              " 30172,\n",
              " 30171,\n",
              " 30170,\n",
              " 30169,\n",
              " 30168,\n",
              " 30167,\n",
              " 30166,\n",
              " 30165,\n",
              " 30164,\n",
              " 30163,\n",
              " 30162,\n",
              " 30161,\n",
              " 30160,\n",
              " 30159,\n",
              " 30158,\n",
              " 30157,\n",
              " 30156,\n",
              " 30155,\n",
              " 30154,\n",
              " 30153,\n",
              " 30152,\n",
              " 30151,\n",
              " 30150,\n",
              " 30149,\n",
              " 30148,\n",
              " 30147,\n",
              " 30146,\n",
              " 30145,\n",
              " 30144,\n",
              " 30143,\n",
              " 30142,\n",
              " 30141,\n",
              " 30140,\n",
              " 30139,\n",
              " 30138,\n",
              " 30137,\n",
              " 30136,\n",
              " 30135,\n",
              " 30134,\n",
              " 30133,\n",
              " 30132,\n",
              " 30131,\n",
              " 30130,\n",
              " 30129,\n",
              " 30128,\n",
              " 30127,\n",
              " 30126,\n",
              " 30125,\n",
              " 30124,\n",
              " 30123,\n",
              " 30122,\n",
              " 30121,\n",
              " 30120,\n",
              " 30119,\n",
              " 30118,\n",
              " 30117,\n",
              " 30116,\n",
              " 30115,\n",
              " 30114,\n",
              " 30113,\n",
              " 30112,\n",
              " 30111,\n",
              " 30110,\n",
              " 30109,\n",
              " 30108,\n",
              " 30107,\n",
              " 30106,\n",
              " 30105,\n",
              " 30104,\n",
              " 30103,\n",
              " 30102,\n",
              " 30101,\n",
              " 30100,\n",
              " 30099,\n",
              " 30098,\n",
              " 30097,\n",
              " 30096,\n",
              " 30095,\n",
              " 30094,\n",
              " 30093,\n",
              " 30092,\n",
              " 30091,\n",
              " 30090,\n",
              " 30089,\n",
              " 30088,\n",
              " 30087,\n",
              " 30086,\n",
              " 30085,\n",
              " 30084,\n",
              " 30083,\n",
              " 30082,\n",
              " 30081,\n",
              " 30080,\n",
              " 30079,\n",
              " 30078,\n",
              " 30077,\n",
              " 30076,\n",
              " 30075,\n",
              " 30074,\n",
              " 30073,\n",
              " 30072,\n",
              " 30071,\n",
              " 30070,\n",
              " 30069,\n",
              " 30068,\n",
              " 30067,\n",
              " 30066,\n",
              " 30065,\n",
              " 30064,\n",
              " 30063,\n",
              " 30062,\n",
              " 30061,\n",
              " 30060,\n",
              " 30059,\n",
              " 30058,\n",
              " 30057,\n",
              " 30056,\n",
              " 30055,\n",
              " 30054,\n",
              " 30053,\n",
              " 30052,\n",
              " 30051,\n",
              " 30050,\n",
              " 30049,\n",
              " 30048,\n",
              " 30047,\n",
              " 30046,\n",
              " 30045,\n",
              " 30044,\n",
              " 30043,\n",
              " 30042,\n",
              " 30041,\n",
              " 30040,\n",
              " 30039,\n",
              " 30038,\n",
              " 30037,\n",
              " 30036,\n",
              " 30035,\n",
              " 30034,\n",
              " 30033,\n",
              " 30032,\n",
              " 30031,\n",
              " 30030,\n",
              " 30029,\n",
              " 30028,\n",
              " 30027,\n",
              " 30026,\n",
              " 30025,\n",
              " 30024,\n",
              " 30023,\n",
              " 30022,\n",
              " 30021,\n",
              " 30020,\n",
              " 30019,\n",
              " 30018,\n",
              " 30017,\n",
              " 30016,\n",
              " 30015,\n",
              " 30014,\n",
              " 30013,\n",
              " 30012,\n",
              " 30011,\n",
              " 30010,\n",
              " 30009,\n",
              " 30008,\n",
              " 30007,\n",
              " 30006,\n",
              " 30005,\n",
              " 30004,\n",
              " 30003,\n",
              " 30002,\n",
              " 30001,\n",
              " 30000,\n",
              " 29999,\n",
              " 29998,\n",
              " 29997,\n",
              " 29996,\n",
              " 29995,\n",
              " 29994,\n",
              " 29993,\n",
              " 29992,\n",
              " 29991,\n",
              " 29990,\n",
              " 29989,\n",
              " 29988,\n",
              " 29987,\n",
              " 29986,\n",
              " 29985,\n",
              " 29984,\n",
              " 29983,\n",
              " 29982,\n",
              " 29981,\n",
              " 29980,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "U8_qKRlyGAcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85658e1b-8371-4942-85ad-ec05eb1de130"
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH=0\n",
        "for i in range(x_train.shape[0]):\n",
        "    if len(list(x_train)[i])>MAX_SEQUENCE_LENGTH:\n",
        "        MAX_SEQUENCE_LENGTH=len(list(x_train)[i])\n",
        "MAX_SEQUENCE_LENGTH"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "kvbc3GyOEq4T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmNUJvbzdHyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgkhMwZdKNmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdf22e8d-543d-4589-c8e9-591e415d2c9a"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 2376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ZQ18Kvy1-PZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word embedding"
      ]
    },
    {
      "metadata": {
        "id": "-1FYWO5KPA7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6235cf5d-5e4f-4954-d0fd-6203011862bc"
      },
      "cell_type": "code",
      "source": [
        "! pip install pydrive\n",
        "# these classes allow you to request the Google drive API\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = '1QhbmVQ3uoEmNvcjCViUK1ZZe0ErRjoEf'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "# allows you to temporarily load your file in the notebook VM\n",
        "\n",
        "# assume the file is called file.csv and it's located at the root of your drive\n",
        "downloaded.GetContentFile('glove.6B.100d.txt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (3.4.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.2)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Running setup.py bdist_wheel for pydrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E6lbzgdUAJSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eceb29a6-a194-43d5-9670-6ca7d9964b01"
      },
      "cell_type": "code",
      "source": [
        "embeddings_index= {}\n",
        "f = open('glove.6B.100d.txt',encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    codes = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = codes\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 400000 word vectors in Glove 6B 100d.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tkEQc-ftJmw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_len=len(word_index)+1\n",
        "emb_dim=embeddings_index.get('one').shape[0]\n",
        "emb_matrix=np.random.random((vocab_len,emb_dim))\n",
        "for word, index in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        emb_matrix[index,:] = embedding_vector\n",
        "embedding_layer = Embedding(vocab_len, emb_dim, input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "embedding_layer.build((None,))\n",
        "embedding_layer.set_weights([emb_matrix])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8UBuZ7FtgFTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6b8df28-f949-4896-a6ad-8d3fec99af29"
      },
      "cell_type": "code",
      "source": [
        "emb_matrix.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30980, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "jlCV9wck-V2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building and compiling the model"
      ]
    },
    {
      "metadata": {
        "id": "ximQVhh-JyqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "affed3c2-8074-4c34-906c-989bb48259f6"
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embeddings = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, 3)(embeddings)\n",
        "x = BatchNormalization(axis=2)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "\n",
        "x = Conv1D(256, 3)(x)\n",
        "x = BatchNormalization(axis=2)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "\n",
        "x = Conv1D(512, 3)(x)\n",
        "x = BatchNormalization(axis=2)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(3)(x)  \n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(nb_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, x)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"Simplified convolutional neural network\")\n",
        "model.summary()\n",
        "cp=ModelCheckpoint('textClassification_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simplified convolutional neural network\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 2376)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 2376, 100)         3098000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 2374, 128)         38528     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 2374, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2374, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 791, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 789, 256)          98560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 789, 256)          1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 789, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 263, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 261, 512)          393728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 261, 512)          2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 261, 512)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 87, 512)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 44544)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               5701760   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                5934      \n",
            "=================================================================\n",
            "Total params: 9,340,094\n",
            "Trainable params: 9,338,302\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1aUk0WLw-dKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Eliminating words out of word index dictionary"
      ]
    },
    {
      "metadata": {
        "id": "VRr5wE8-bkso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "40c7d380-ffdf-4f1f-bdf0-76fb429a98ea"
      },
      "cell_type": "code",
      "source": [
        "for i in range(x_train.shape[0]):\n",
        "    for j in range(x_train.shape[1]):\n",
        "        if x_train[i,j]>=30980:\n",
        "            print(x_train[i,j])\n",
        "            print (i,j)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30981\n",
            "713 2077\n",
            "30980\n",
            "5886 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uq7lILg9bksu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train[713, 2077]=0\n",
        "x_train[5886, 2019]=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y3XAVyGu-lJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ]
    },
    {
      "metadata": {
        "id": "OkF2EUk-MlOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "a17c61ad-f374-414f-da55-eb339512f750"
      },
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=50, batch_size=32,callbacks=[cp])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/50\n",
            "8982/8982 [==============================] - 34s 4ms/step - loss: 5.3513 - acc: 0.3955 - val_loss: 6.5665 - val_acc: 0.3615\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.36153, saving model to textClassification_cnn.hdf5\n",
            "Epoch 2/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 2.5326 - acc: 0.4582 - val_loss: 2.4715 - val_acc: 0.4595\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.36153 to 0.45948, saving model to textClassification_cnn.hdf5\n",
            "Epoch 3/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 2.2610 - acc: 0.4423 - val_loss: 2.5680 - val_acc: 0.3882\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.45948\n",
            "Epoch 4/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 2.1294 - acc: 0.4561 - val_loss: 2.1431 - val_acc: 0.4386\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.45948\n",
            "Epoch 5/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 2.0084 - acc: 0.4690 - val_loss: 4.2698 - val_acc: 0.3740\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.45948\n",
            "Epoch 6/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.9313 - acc: 0.4708 - val_loss: 2.3364 - val_acc: 0.4020\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.45948\n",
            "Epoch 7/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.8471 - acc: 0.5046 - val_loss: 1.9309 - val_acc: 0.5361\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.45948 to 0.53606, saving model to textClassification_cnn.hdf5\n",
            "Epoch 8/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.8051 - acc: 0.5226 - val_loss: 2.1707 - val_acc: 0.4657\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.53606\n",
            "Epoch 9/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.8021 - acc: 0.5266 - val_loss: 2.3011 - val_acc: 0.4987\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.53606\n",
            "Epoch 10/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.7514 - acc: 0.5434 - val_loss: 2.1643 - val_acc: 0.4452\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.53606\n",
            "Epoch 11/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.7157 - acc: 0.5524 - val_loss: 2.1943 - val_acc: 0.5472\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.53606 to 0.54720, saving model to textClassification_cnn.hdf5\n",
            "Epoch 12/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.6789 - acc: 0.5682 - val_loss: 2.4525 - val_acc: 0.3170\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.54720\n",
            "Epoch 13/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.6419 - acc: 0.5755 - val_loss: 2.7276 - val_acc: 0.2458\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.54720\n",
            "Epoch 14/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.6182 - acc: 0.5818 - val_loss: 1.8617 - val_acc: 0.5904\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.54720 to 0.59038, saving model to textClassification_cnn.hdf5\n",
            "Epoch 15/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.6089 - acc: 0.5823 - val_loss: 1.8737 - val_acc: 0.5450\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.59038\n",
            "Epoch 16/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5829 - acc: 0.5933 - val_loss: 1.9243 - val_acc: 0.5597\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.59038\n",
            "Epoch 17/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5723 - acc: 0.5946 - val_loss: 2.4660 - val_acc: 0.5708\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59038\n",
            "Epoch 18/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5523 - acc: 0.5993 - val_loss: 1.8852 - val_acc: 0.5917\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.59038 to 0.59172, saving model to textClassification_cnn.hdf5\n",
            "Epoch 19/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5586 - acc: 0.5984 - val_loss: 1.9374 - val_acc: 0.5788\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.59172\n",
            "Epoch 20/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5550 - acc: 0.6006 - val_loss: 1.9151 - val_acc: 0.5347\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.59172\n",
            "Epoch 21/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5417 - acc: 0.6000 - val_loss: 1.9241 - val_acc: 0.5757\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.59172\n",
            "Epoch 22/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5140 - acc: 0.6071 - val_loss: 2.6377 - val_acc: 0.4052\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.59172\n",
            "Epoch 23/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.5255 - acc: 0.6050 - val_loss: 1.8645 - val_acc: 0.5476\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.59172\n",
            "Epoch 24/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4922 - acc: 0.6128 - val_loss: 1.9527 - val_acc: 0.5539\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.59172\n",
            "Epoch 25/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4814 - acc: 0.6152 - val_loss: 1.8453 - val_acc: 0.5565\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.59172\n",
            "Epoch 26/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4858 - acc: 0.6138 - val_loss: 1.9813 - val_acc: 0.5922\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.59172 to 0.59216, saving model to textClassification_cnn.hdf5\n",
            "Epoch 27/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4824 - acc: 0.6156 - val_loss: 1.9661 - val_acc: 0.5530\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.59216\n",
            "Epoch 28/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4734 - acc: 0.6143 - val_loss: 2.2543 - val_acc: 0.5276\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.59216\n",
            "Epoch 29/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4577 - acc: 0.6196 - val_loss: 1.9137 - val_acc: 0.5859\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.59216\n",
            "Epoch 30/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4457 - acc: 0.6218 - val_loss: 2.1544 - val_acc: 0.5761\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.59216\n",
            "Epoch 31/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4556 - acc: 0.6205 - val_loss: 1.9265 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.59216\n",
            "Epoch 32/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4397 - acc: 0.6196 - val_loss: 1.9560 - val_acc: 0.5472\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.59216\n",
            "Epoch 33/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4315 - acc: 0.6239 - val_loss: 1.9484 - val_acc: 0.5708\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.59216\n",
            "Epoch 34/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4191 - acc: 0.6257 - val_loss: 1.8585 - val_acc: 0.5801\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.59216\n",
            "Epoch 35/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4068 - acc: 0.6280 - val_loss: 2.1059 - val_acc: 0.5739\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.59216\n",
            "Epoch 36/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4056 - acc: 0.6283 - val_loss: 1.9474 - val_acc: 0.5668\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.59216\n",
            "Epoch 37/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3941 - acc: 0.6358 - val_loss: 1.9421 - val_acc: 0.5356\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.59216\n",
            "Epoch 38/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3978 - acc: 0.6303 - val_loss: 2.2738 - val_acc: 0.4639\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.59216\n",
            "Epoch 39/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3961 - acc: 0.6316 - val_loss: 2.5365 - val_acc: 0.5984\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.59216 to 0.59840, saving model to textClassification_cnn.hdf5\n",
            "Epoch 40/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3969 - acc: 0.6323 - val_loss: 1.8260 - val_acc: 0.5735\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.59840\n",
            "Epoch 41/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.4055 - acc: 0.6319 - val_loss: 1.8617 - val_acc: 0.5735\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.59840\n",
            "Epoch 42/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3863 - acc: 0.6313 - val_loss: 1.8681 - val_acc: 0.5579\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.59840\n",
            "Epoch 43/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3735 - acc: 0.6355 - val_loss: 2.3628 - val_acc: 0.4817\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.59840\n",
            "Epoch 44/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3855 - acc: 0.6343 - val_loss: 1.9340 - val_acc: 0.5597\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.59840\n",
            "Epoch 45/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3621 - acc: 0.6388 - val_loss: 1.9793 - val_acc: 0.5686\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.59840\n",
            "Epoch 46/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3730 - acc: 0.6389 - val_loss: 2.5902 - val_acc: 0.3522\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.59840\n",
            "Epoch 47/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3679 - acc: 0.6409 - val_loss: 1.8868 - val_acc: 0.5712\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.59840\n",
            "Epoch 48/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3514 - acc: 0.6425 - val_loss: 1.8500 - val_acc: 0.5775\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.59840\n",
            "Epoch 49/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3616 - acc: 0.6396 - val_loss: 1.9288 - val_acc: 0.5735\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.59840\n",
            "Epoch 50/50\n",
            "8982/8982 [==============================] - 31s 3ms/step - loss: 1.3478 - acc: 0.6416 - val_loss: 2.0998 - val_acc: 0.5766\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.59840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dLYEQSWqOYiU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM model"
      ]
    },
    {
      "metadata": {
        "id": "QQx9F1Bpc4Rf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "dcfc450d-f5cb-44aa-e728-e9283ab6f43a"
      },
      "cell_type": "code",
      "source": [
        "y = LSTM(128, return_sequences=True)(embeddings)\n",
        "y = Dropout(0.5)(y)\n",
        "\n",
        "#y = LSTM(128, return_sequences=True)(y)\n",
        "#y = Dropout(0.5)(y)\n",
        "\n",
        "y = LSTM(128, return_sequences=False)(y)\n",
        "y = Dropout(0.5)(y)\n",
        "\n",
        "y = Dense(nb_classes, activation='softmax')(y)\n",
        "\n",
        "model_rnn = Model(sequence_input, y)\n",
        "model_rnn.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"Simplified recurrent neural network\")\n",
        "model_rnn.summary()\n",
        "cp_rnn=ModelCheckpoint('textClassification_rnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simplified recurrent neural network\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 2376)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 2376, 100)         3098000   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 2376, 128)         117248    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2376, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 46)                5934      \n",
            "=================================================================\n",
            "Total params: 3,352,766\n",
            "Trainable params: 3,352,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lut2di4Wbks8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "14f52363-0669-469c-a683-36a37572dc7b"
      },
      "cell_type": "code",
      "source": [
        "history_rnn=model_rnn.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=5, batch_size=16,callbacks=[cp_rnn])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/5\n",
            "8982/8982 [==============================] - 3089s 344ms/step - loss: 2.2149 - acc: 0.4495 - val_loss: 2.0020 - val_acc: 0.5196\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.51959, saving model to textClassification_rnn.hdf5\n",
            "Epoch 2/5\n",
            "8982/8982 [==============================] - 3088s 344ms/step - loss: 2.0075 - acc: 0.4973 - val_loss: 1.8286 - val_acc: 0.5436\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.51959 to 0.54363, saving model to textClassification_rnn.hdf5\n",
            "Epoch 3/5\n",
            "8982/8982 [==============================] - 3076s 342ms/step - loss: 1.9778 - acc: 0.4921 - val_loss: 1.8048 - val_acc: 0.5294\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.54363\n",
            "Epoch 4/5\n",
            "8982/8982 [==============================] - 3082s 343ms/step - loss: 1.8440 - acc: 0.5185 - val_loss: 1.7721 - val_acc: 0.5419\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.54363\n",
            "Epoch 5/5\n",
            "8982/8982 [==============================] - 3074s 342ms/step - loss: 1.8064 - acc: 0.5401 - val_loss: 2.0552 - val_acc: 0.4920\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.54363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_lycjMUPbktA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}